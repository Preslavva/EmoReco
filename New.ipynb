{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-05T13:15:50.102395Z",
     "start_time": "2026-01-05T13:15:50.050277Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"modelling_metadata.csv\")\n",
    "df.head()\n",
    "df.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7431, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:32:21.236292Z",
     "start_time": "2026-01-04T19:32:21.226581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Audio settings\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_SEC = 2.5\n",
    "TARGET_SAMPLES = int(SAMPLE_RATE * DURATION_SEC)\n",
    "\n",
    "# Spectrogram settings (speech-friendly)\n",
    "N_FFT = 512\n",
    "WIN_LENGTH = 400      # 25 ms at 16 kHz\n",
    "HOP_LENGTH = 160      # 10 ms at 16 kHz\n",
    "N_MELS = 80\n",
    "\n",
    "# Expected time frames (approx; librosa can differ by 1 frame depending on centering)\n",
    "TARGET_FRAMES = 1 + (TARGET_SAMPLES // HOP_LENGTH)\n",
    "\n",
    "# Output folders\n",
    "OUT_NPY_DIR = \"mel_new_npy\"\n",
    "os.makedirs(OUT_NPY_DIR, exist_ok=True)\n"
   ],
   "id": "cdc794bb9a865243",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:32:23.004132Z",
     "start_time": "2026-01-04T19:32:22.994515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def load_and_fix_length_train(path, sample_rate=SAMPLE_RATE, target_samples=TARGET_SAMPLES, top_db=25):\n",
    "    y, sr = librosa.load(path, sr=None, mono=True)\n",
    "    if sr != sample_rate:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=sample_rate)\n",
    "\n",
    "    y, _ = librosa.effects.trim(y, top_db=top_db)\n",
    "\n",
    "    if len(y) >= target_samples:\n",
    "        start = np.random.randint(0, len(y) - target_samples + 1)\n",
    "        y = y[start:start + target_samples]\n",
    "    else:\n",
    "        if len(y) == 0:\n",
    "            y = np.zeros(target_samples, dtype=np.float32)\n",
    "        else:\n",
    "            reps = int(np.ceil(target_samples / len(y)))\n",
    "            y_rep = np.tile(y, reps)\n",
    "            start = np.random.randint(0, len(y))\n",
    "            y = y_rep[start:start + target_samples]\n",
    "\n",
    "    # ✅ FINAL GUARANTEE (should already be true, but makes it bulletproof)\n",
    "    if len(y) != target_samples:\n",
    "        y = y[:target_samples] if len(y) > target_samples else np.pad(y, (0, target_samples - len(y)), mode=\"reflect\")\n",
    "\n",
    "    return y.astype(np.float32)\n"
   ],
   "id": "2ff2ae7decef9e72",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:32:25.109464Z",
     "start_time": "2026-01-04T19:32:25.104138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def load_and_fix_length_eval(path, sample_rate=SAMPLE_RATE, target_samples=TARGET_SAMPLES, top_db=25):\n",
    "    y, sr = librosa.load(path, sr=None, mono=True)\n",
    "    if sr != sample_rate:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=sample_rate)\n",
    "\n",
    "    y, _ = librosa.effects.trim(y, top_db=top_db)\n",
    "\n",
    "    if len(y) >= target_samples:\n",
    "        start = (len(y) - target_samples) // 2\n",
    "        y = y[start:start + target_samples]\n",
    "    else:\n",
    "        if len(y) == 0:\n",
    "            y = np.zeros(target_samples, dtype=np.float32)\n",
    "        else:\n",
    "            y = np.pad(y, (0, target_samples - len(y)), mode=\"reflect\")\n",
    "\n",
    "    # ✅ FINAL GUARANTEE\n",
    "    if len(y) != target_samples:\n",
    "        y = y[:target_samples] if len(y) > target_samples else np.pad(y, (0, target_samples - len(y)), mode=\"reflect\")\n",
    "\n",
    "    return y.astype(np.float32)\n"
   ],
   "id": "568601adca588464",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:32:27.237947Z",
     "start_time": "2026-01-04T19:32:27.232363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def wav_to_logmel(y, sample_rate, n_fft, win_length, hop_length, n_mels, target_frames, eps=1e-6):\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        power=2.0,\n",
    "    )\n",
    "\n",
    "    # log compression\n",
    "    S = np.log(S + eps)\n",
    "\n",
    "    # CMVN per mel bin over time\n",
    "    mean = S.mean(axis=1, keepdims=True)\n",
    "    std  = S.std(axis=1, keepdims=True) + eps\n",
    "    S = (S - mean) / std\n",
    "\n",
    "    # Force fixed time dimension (T)\n",
    "    T = S.shape[1]\n",
    "    if T < target_frames:\n",
    "        S = np.pad(S, ((0, 0), (0, target_frames - T)), mode=\"constant\")\n",
    "    elif T > target_frames:\n",
    "        S = S[:, :target_frames]\n",
    "\n",
    "    return S[np.newaxis, :, :].astype(np.float32)\n"
   ],
   "id": "bfcd2676f63a8304",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:16:36.955986Z",
     "start_time": "2026-01-05T13:16:36.945686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ],
   "id": "75acc9f154b77797",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:16:31.812187Z",
     "start_time": "2026-01-05T13:16:21.950331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "import random, numpy as np, torch\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Emotion mapping (keep as-is)\n",
    "EMO_MAP = {\n",
    "    \"ANG\": 0,\n",
    "    \"DIS\": 1,\n",
    "    \"FEA\": 2,\n",
    "    \"HAP\": 3,\n",
    "    \"NEU\": 4,\n",
    "    \"SAD\": 5\n",
    "}\n",
    "NUM_CLASSES = len(EMO_MAP)\n",
    "\n",
    "# Data locations (separate per split)\n",
    "TRAIN_NPY_DIR = \"mel_npy_train\"\n",
    "VAL_NPY_DIR   = \"mel_npy_val\"\n",
    "TEST_NPY_DIR  = \"mel_npy_test\"\n",
    "\n",
    "import os\n",
    "os.makedirs(TRAIN_NPY_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_NPY_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_NPY_DIR, exist_ok=True)\n"
   ],
   "id": "d7a209e95f6d4537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:16:40.495857Z",
     "start_time": "2026-01-05T13:16:40.446742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Example: EMO_MAP = {\"ANG\": \"angry\", \"DIS\": \"disgust\", \"FEA\": \"fear\", \"HAP\": \"happy\", \"NEU\": \"neutral\", \"SAD\": \"sad\"}\n",
    "# Make sure EMO_MAP exists in your code.\n",
    "\n",
    "def parse_actor_and_emotion(filename: str):\n",
    "    \"\"\"\n",
    "    Parses CREMA-D-like filenames such as: 1001_IEO_HAP_HI.wav\n",
    "    Returns: (actor_id, emotion_label)\n",
    "    \"\"\"\n",
    "    # Actor ID: prefer 4 digits at start followed by underscore\n",
    "    m = re.match(r\"^(\\d{4})_\", filename)\n",
    "    if m is not None:\n",
    "        actor_id = m.group(1)\n",
    "    else:\n",
    "        # fallback: first run of digits anywhere\n",
    "        m2 = re.search(r\"\\d+\", filename)\n",
    "        if m2 is None:\n",
    "            raise ValueError(f\"No actor ID in {filename}\")\n",
    "        actor_id = m2.group(0)\n",
    "\n",
    "    # Emotion code -> mapped label\n",
    "    emotion = None\n",
    "    for emo_code, emo_label in EMO_MAP.items():\n",
    "        if f\"_{emo_code}_\" in filename or filename.startswith(f\"{emo_code}_\"):\n",
    "            emotion = emo_label\n",
    "            break\n",
    "\n",
    "    if emotion is None:\n",
    "        raise ValueError(f\"No emotion code in {filename}\")\n",
    "\n",
    "    return actor_id, emotion\n",
    "\n",
    "\n",
    "def actor_independent_split(paths, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Speaker-independent split: all files from an actor go into the same split.\n",
    "    Returns:\n",
    "      train_paths, val_paths, test_paths, train_actors, val_actors, test_actors\n",
    "    \"\"\"\n",
    "    if train_ratio < 0 or val_ratio < 0 or (train_ratio + val_ratio) > 1.0:\n",
    "        raise ValueError(\"train_ratio and val_ratio must be >= 0 and sum to <= 1.0\")\n",
    "\n",
    "    by_actor = {}\n",
    "    for p in paths:\n",
    "        fname = os.path.basename(p)\n",
    "        actor_id, _ = parse_actor_and_emotion(fname)\n",
    "        by_actor.setdefault(actor_id, []).append(p)\n",
    "\n",
    "    actors = sorted(by_actor.keys())\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(actors)\n",
    "\n",
    "    n = len(actors)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train_actors = set(actors[:n_train])\n",
    "    val_actors   = set(actors[n_train:n_train + n_val])\n",
    "    test_actors  = set(actors[n_train + n_val:])\n",
    "\n",
    "    def collect(actor_set):\n",
    "        out = []\n",
    "        for a in actor_set:\n",
    "            out.extend(by_actor[a])\n",
    "        return out\n",
    "\n",
    "    train_paths = collect(train_actors)\n",
    "    val_paths   = collect(val_actors)\n",
    "    test_paths  = collect(test_actors)\n",
    "\n",
    "    return train_paths, val_paths, test_paths, train_actors, val_actors, test_actors\n",
    "\n",
    "\n",
    "# ----- Example usage -----\n",
    "train_paths, val_paths, test_paths, *_ = actor_independent_split(df[\"file_path\"], seed=42)\n",
    "print(len(train_paths), len(val_paths), len(test_paths))\n"
   ],
   "id": "7da7d92502651ea2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5141 1066 1224\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:32:59.786948Z",
     "start_time": "2026-01-04T19:32:35.644545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Output folders (separate per split)\n",
    "TRAIN_NPY_DIR = \"mel_npy_train\"\n",
    "VAL_NPY_DIR   = \"mel_npy_val\"\n",
    "TEST_NPY_DIR  = \"mel_npy_test\"\n",
    "os.makedirs(TRAIN_NPY_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_NPY_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_NPY_DIR, exist_ok=True)\n",
    "\n",
    "LOG_EVERY = 500\n",
    "\n",
    "def precompute_mels(paths, out_dir, load_fn):\n",
    "    for i, wav_path in enumerate(paths, start=1):\n",
    "        file_name = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "        out_path = os.path.join(out_dir, f\"{file_name}.npy\")\n",
    "\n",
    "        try:\n",
    "            y = load_fn(wav_path)  # train or eval length handling\n",
    "\n",
    "            # ✅ pass params explicitly + enforce fixed TARGET_FRAMES inside wav_to_logmel\n",
    "            mel = wav_to_logmel(\n",
    "                y,\n",
    "                SAMPLE_RATE,\n",
    "                N_FFT,\n",
    "                WIN_LENGTH,\n",
    "                HOP_LENGTH,\n",
    "                N_MELS,\n",
    "                TARGET_FRAMES\n",
    "            )\n",
    "\n",
    "            np.save(out_path, mel)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping: {wav_path}\\n  -> {e}\")\n",
    "            continue\n",
    "\n",
    "        if i % LOG_EVERY == 0:\n",
    "            print(f\"[{out_dir}] Processed {i} files...\")\n",
    "\n",
    "    print(f\"Done. Saved {len(paths)} files to: {out_dir}\")\n",
    "\n",
    "\n",
    "# 1) Make the split (speaker-independent)\n",
    "train_paths, val_paths, test_paths, *_ = actor_independent_split(\n",
    "    df[\"file_path\"].tolist(),\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(\"Split sizes:\", len(train_paths), len(val_paths), len(test_paths))\n",
    "\n",
    "# 2) Precompute\n",
    "print(\"Precomputing TRAIN mels...\")\n",
    "precompute_mels(train_paths, TRAIN_NPY_DIR, load_and_fix_length_train)\n",
    "\n",
    "print(\"Precomputing VAL mels...\")\n",
    "precompute_mels(val_paths, VAL_NPY_DIR, load_and_fix_length_eval)\n",
    "\n",
    "print(\"Precomputing TEST mels...\")\n",
    "precompute_mels(test_paths, TEST_NPY_DIR, load_and_fix_length_eval)\n"
   ],
   "id": "fc5e7023f552b395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 5141 1066 1224\n",
      "Precomputing TRAIN mels...\n",
      "[mel_npy_train] Processed 500 files...\n",
      "[mel_npy_train] Processed 1000 files...\n",
      "[mel_npy_train] Processed 1500 files...\n",
      "[mel_npy_train] Processed 2000 files...\n",
      "[mel_npy_train] Processed 2500 files...\n",
      "[mel_npy_train] Processed 3000 files...\n",
      "[mel_npy_train] Processed 3500 files...\n",
      "[mel_npy_train] Processed 4000 files...\n",
      "[mel_npy_train] Processed 4500 files...\n",
      "[mel_npy_train] Processed 5000 files...\n",
      "Done. Saved 5141 files to: mel_npy_train\n",
      "Precomputing VAL mels...\n",
      "[mel_npy_val] Processed 500 files...\n",
      "[mel_npy_val] Processed 1000 files...\n",
      "Done. Saved 1066 files to: mel_npy_val\n",
      "Precomputing TEST mels...\n",
      "[mel_npy_test] Processed 500 files...\n",
      "[mel_npy_test] Processed 1000 files...\n",
      "Done. Saved 1224 files to: mel_npy_test\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:33:01.955798Z",
     "start_time": "2026-01-04T19:33:01.930361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np, os\n",
    "fn = next(f for f in os.listdir(TRAIN_NPY_DIR) if f.endswith(\".npy\"))\n",
    "x = np.load(os.path.join(TRAIN_NPY_DIR, fn))\n",
    "print(x.shape)  # should be (1, 80, TARGET_FRAMES)\n"
   ],
   "id": "c7b99e48a23f763d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 80, 251)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:34:40.989845Z",
     "start_time": "2026-01-04T19:34:40.985622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MelNPYDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        fname = os.path.basename(path)\n",
    "\n",
    "        # loading spectrogram\n",
    "        spec = np.load(path).astype(np.float32)\n",
    "        if spec.ndim == 2:\n",
    "            spec = spec[np.newaxis, :, :]\n",
    "\n",
    "        # parsing label from filename\n",
    "        _, label = parse_actor_and_emotion(fname)\n",
    "\n",
    "        x = torch.from_numpy(spec)\n",
    "        y = torch.tensor(label, dtype=torch.long)\n",
    "        return x, y\n"
   ],
   "id": "c57d1a80cdcc0e7f",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:34:42.846414Z",
     "start_time": "2026-01-04T19:34:42.840482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def wav_paths_to_npy_paths(wav_paths, npy_dir):\n",
    "    out = []\n",
    "    for wav_path in wav_paths:\n",
    "        base = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "        out.append(os.path.join(npy_dir, base + \".npy\"))\n",
    "    return out\n"
   ],
   "id": "affe35d4eede3a4a",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:34:44.476865Z",
     "start_time": "2026-01-04T19:34:44.439995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_npy_paths = wav_paths_to_npy_paths(train_paths, TRAIN_NPY_DIR)\n",
    "val_npy_paths   = wav_paths_to_npy_paths(val_paths, VAL_NPY_DIR)\n",
    "test_npy_paths  = wav_paths_to_npy_paths(test_paths, TEST_NPY_DIR)\n",
    "\n",
    "train_ds = MelNPYDataset(train_npy_paths)\n",
    "val_ds   = MelNPYDataset(val_npy_paths)\n",
    "test_ds  = MelNPYDataset(test_npy_paths)\n"
   ],
   "id": "dd0ad964b7f15dc6",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:34:46.187596Z",
     "start_time": "2026-01-04T19:34:46.177708Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_ds[0][0].shape, train_ds[0][1])\n",
   "id": "de6bdcd9bf591e80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 251]) tensor(0)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:34:48.132678Z",
     "start_time": "2026-01-04T19:34:48.124817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    MelNPYDataset(train_npy_paths),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    MelNPYDataset(val_npy_paths),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    MelNPYDataset(test_npy_paths),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ],
   "id": "d38bf7af2ba799fa",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:34:49.731552Z",
     "start_time": "2026-01-04T19:34:49.711631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "print(xb.shape, yb[:5])\n"
   ],
   "id": "b491e2aea5500505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 80, 251]) tensor([5, 5, 0, 2, 0])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:37:43.122721Z",
     "start_time": "2026-01-04T19:37:43.112456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNBiLSTM(nn.Module):\n",
    "    def __init__(self, lstm_hidden=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1)),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1))\n",
    "        )\n",
    "\n",
    "        # Use your actual fixed spectrogram size:\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, N_MELS, TARGET_FRAMES)\n",
    "            z = self.cnn(dummy)\n",
    "            C, Fp, Tp = z.shape[1], z.shape[2], z.shape[3]\n",
    "            lstm_in = C * Fp\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_in,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # mean+std pooling doubles the feature size -> 4H instead of 2H\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4 * lstm_hidden, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)                 # (B, C, F, T)\n",
    "        z = z.permute(0, 3, 1, 2)       # (B, T, C, F)\n",
    "        z = z.flatten(2)                # (B, T, C*F)\n",
    "        out, _ = self.lstm(z)           # (B, T, 2H)\n",
    "\n",
    "        # Stats pooling over time (stronger than mean only)\n",
    "        mu = out.mean(dim=1)            # (B, 2H)\n",
    "        sigma = out.std(dim=1)          # (B, 2H)\n",
    "        pooled = torch.cat([mu, sigma], dim=1)  # (B, 4H)\n",
    "\n",
    "        return self.fc(pooled)\n"
   ],
   "id": "b44385fbebf1d0ed",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T19:42:37.936137Z",
     "start_time": "2026-01-04T19:42:37.928477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_true.append(y.detach().cpu().numpy())\n",
    "\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_true)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return acc, macro_f1, cm\n",
    "\n"
   ],
   "id": "a6e563ecc813049b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:25:22.346782Z",
     "start_time": "2026-01-04T19:42:40.452246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CNNBiLSTM(lstm_hidden=128).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=2, min_lr=1e-6\n",
    ")\n",
    "\n",
    "max_epochs = 120\n",
    "early_patience = 8\n",
    "best_val = -1.0\n",
    "wait = 0\n",
    "best_path = \"best_cremad.pt\"\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_acc, val_f1, _ = evaluate(model, val_loader)\n",
    "\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"Epoch {epoch:03d} | train_loss {train_loss:.4f} | val_acc {val_acc:.4f} | val_f1 {val_f1:.4f} | lr {lr:.2e}\")\n",
    "\n",
    "    if val_f1 > best_val + 1e-4:\n",
    "        best_val = val_f1\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= early_patience:\n",
    "            print(f\"Early stopping. Best val_f1={best_val:.4f}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "best_acc, best_f1, cm = evaluate(model, val_loader)\n",
    "print(f\"BEST CHECKPOINT | val_acc={best_acc:.4f} | val_macro_f1={best_f1:.4f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ],
   "id": "c62baa0f0545ad1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss 1.6105 | val_acc 0.4071 | val_f1 0.3455 | lr 3.00e-04\n",
      "Epoch 002 | train_loss 1.5386 | val_acc 0.4212 | val_f1 0.3707 | lr 3.00e-04\n",
      "Epoch 003 | train_loss 1.5078 | val_acc 0.4634 | val_f1 0.4262 | lr 3.00e-04\n",
      "Epoch 004 | train_loss 1.4788 | val_acc 0.4794 | val_f1 0.4526 | lr 3.00e-04\n",
      "Epoch 005 | train_loss 1.4546 | val_acc 0.4475 | val_f1 0.4096 | lr 3.00e-04\n",
      "Epoch 006 | train_loss 1.4259 | val_acc 0.5019 | val_f1 0.4825 | lr 3.00e-04\n",
      "Epoch 007 | train_loss 1.4073 | val_acc 0.3837 | val_f1 0.3435 | lr 3.00e-04\n",
      "Epoch 008 | train_loss 1.3727 | val_acc 0.4681 | val_f1 0.4455 | lr 3.00e-04\n",
      "Epoch 009 | train_loss 1.3479 | val_acc 0.5291 | val_f1 0.5175 | lr 3.00e-04\n",
      "Epoch 010 | train_loss 1.3098 | val_acc 0.5328 | val_f1 0.5357 | lr 3.00e-04\n",
      "Epoch 011 | train_loss 1.2744 | val_acc 0.5760 | val_f1 0.5701 | lr 3.00e-04\n",
      "Epoch 012 | train_loss 1.2389 | val_acc 0.5816 | val_f1 0.5737 | lr 3.00e-04\n",
      "Epoch 013 | train_loss 1.2182 | val_acc 0.5469 | val_f1 0.5368 | lr 3.00e-04\n",
      "Epoch 014 | train_loss 1.1909 | val_acc 0.5666 | val_f1 0.5556 | lr 3.00e-04\n",
      "Epoch 015 | train_loss 1.1618 | val_acc 0.5553 | val_f1 0.5659 | lr 1.50e-04\n",
      "Epoch 016 | train_loss 1.0903 | val_acc 0.5732 | val_f1 0.5765 | lr 1.50e-04\n",
      "Epoch 017 | train_loss 1.0687 | val_acc 0.5478 | val_f1 0.5394 | lr 1.50e-04\n",
      "Epoch 018 | train_loss 1.0380 | val_acc 0.5826 | val_f1 0.5798 | lr 1.50e-04\n",
      "Epoch 019 | train_loss 1.0096 | val_acc 0.6069 | val_f1 0.6071 | lr 1.50e-04\n",
      "Epoch 020 | train_loss 0.9813 | val_acc 0.5469 | val_f1 0.5424 | lr 1.50e-04\n",
      "Epoch 021 | train_loss 0.9621 | val_acc 0.5779 | val_f1 0.5776 | lr 1.50e-04\n",
      "Epoch 022 | train_loss 0.9320 | val_acc 0.5797 | val_f1 0.5853 | lr 7.50e-05\n",
      "Epoch 023 | train_loss 0.8873 | val_acc 0.6107 | val_f1 0.6138 | lr 7.50e-05\n",
      "Epoch 024 | train_loss 0.8569 | val_acc 0.5901 | val_f1 0.5946 | lr 7.50e-05\n",
      "Epoch 025 | train_loss 0.8338 | val_acc 0.6116 | val_f1 0.6172 | lr 7.50e-05\n",
      "Epoch 026 | train_loss 0.8148 | val_acc 0.5694 | val_f1 0.5685 | lr 7.50e-05\n",
      "Epoch 027 | train_loss 0.8075 | val_acc 0.6154 | val_f1 0.6201 | lr 7.50e-05\n",
      "Epoch 028 | train_loss 0.7744 | val_acc 0.5863 | val_f1 0.5864 | lr 7.50e-05\n",
      "Epoch 029 | train_loss 0.7549 | val_acc 0.6182 | val_f1 0.6190 | lr 7.50e-05\n",
      "Epoch 030 | train_loss 0.7442 | val_acc 0.5760 | val_f1 0.5834 | lr 3.75e-05\n",
      "Epoch 031 | train_loss 0.7113 | val_acc 0.6032 | val_f1 0.6065 | lr 3.75e-05\n",
      "Epoch 032 | train_loss 0.6976 | val_acc 0.5985 | val_f1 0.5997 | lr 3.75e-05\n",
      "Epoch 033 | train_loss 0.6838 | val_acc 0.5704 | val_f1 0.5749 | lr 1.87e-05\n",
      "Epoch 034 | train_loss 0.6715 | val_acc 0.6023 | val_f1 0.6059 | lr 1.87e-05\n",
      "Epoch 035 | train_loss 0.6645 | val_acc 0.5938 | val_f1 0.5977 | lr 1.87e-05\n",
      "Early stopping. Best val_f1=0.6201\n",
      "BEST CHECKPOINT | val_acc=0.6154 | val_macro_f1=0.6201\n",
      "Confusion matrix:\n",
      " [[131  10   6  33   2   0]\n",
      " [  8 101  15  27  12  19]\n",
      " [  6  10 111  23   5  27]\n",
      " [ 12  15  31 109  11   4]\n",
      " [  2   6   3  21 109  15]\n",
      " [  0  18  34  16  19  95]]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:26:10.183690Z",
     "start_time": "2026-01-04T21:25:38.437802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_preds(model, loader, device):\n",
    "    model.eval()\n",
    "    all_y, all_p = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(1).cpu().numpy()\n",
    "        all_p.append(preds)\n",
    "        all_y.append(y.numpy())\n",
    "    return np.concatenate(all_y), np.concatenate(all_p)\n",
    "\n",
    "y_true, y_pred = get_preds(model, test_loader, DEVICE)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[k for k,v in sorted(EMO_MAP.items(), key=lambda x:x[1])]))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ],
   "id": "decef3d7f0cfd5bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.67      0.69      0.68       209\n",
      "         DIS       0.51      0.56      0.53       209\n",
      "         FEA       0.54      0.55      0.54       209\n",
      "         HAP       0.50      0.64      0.56       209\n",
      "         NEU       0.70      0.61      0.65       179\n",
      "         SAD       0.64      0.46      0.54       209\n",
      "\n",
      "    accuracy                           0.58      1224\n",
      "   macro avg       0.59      0.58      0.58      1224\n",
      "weighted avg       0.59      0.58      0.58      1224\n",
      "\n",
      "[[144  28   7  28   2   0]\n",
      " [ 31 116  20  21   7  14]\n",
      " [  7  23 114  41   6  18]\n",
      " [ 20  15  28 133   9   4]\n",
      " [  9  18   3  22 109  18]\n",
      " [  3  26  41  20  22  97]]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:30:25.837920Z",
     "start_time": "2026-01-04T21:30:25.825712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FINAL_PATH = \"cnn_bilstm_emotion_new.pt\"\n",
    "\n",
    "# model should already be the best one (or load best_path first)\n",
    "torch.save(model.state_dict(), FINAL_PATH)\n",
    "print(\"Saved weights to:\", FINAL_PATH)\n"
   ],
   "id": "36c610e3a3293d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights to: cnn_bilstm_emotion_new.pt\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:17:31.566834Z",
     "start_time": "2026-01-05T13:17:31.547868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, re, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cpu\"  # you said CPU-only\n",
    "\n",
    "EMO_MAP = {\"ANG\": 0, \"DIS\": 1, \"FEA\": 2, \"HAP\": 3, \"NEU\": 4, \"SAD\": 5}\n",
    "NUM_CLASSES = len(EMO_MAP)\n",
    "\n",
    "def parse_actor_and_emotion(filename: str):\n",
    "    # filename can end with .wav or .npy; emotion code is still in the basename\n",
    "    m = re.match(r\"^(\\d{4})_\", filename)\n",
    "    if m is None:\n",
    "        raise ValueError(f\"Bad CREMA-D filename: {filename}\")\n",
    "\n",
    "    emotion = None\n",
    "    for emo_code, emo_label in EMO_MAP.items():\n",
    "        if f\"_{emo_code}_\" in filename:\n",
    "            emotion = emo_label\n",
    "            break\n",
    "    if emotion is None:\n",
    "        raise ValueError(f\"No emotion code in {filename}\")\n",
    "    return m.group(1), emotion\n"
   ],
   "id": "14ba57ab0246736e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:17:36.366111Z",
     "start_time": "2026-01-05T13:17:33.576282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_paths, val_paths, test_paths, *_ = actor_independent_split(\n",
    "    df[\"file_path\"].tolist(),\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    seed=SEED\n",
    ")\n",
    "print(len(train_paths), len(val_paths), len(test_paths))\n"
   ],
   "id": "219e9a631c9f8b2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5141 1066 1224\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:55:27.731182Z",
     "start_time": "2026-01-04T21:55:26.810905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, WavLMModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "MODEL_NAME = \"microsoft/wavlm-base\"\n",
    "SAMPLE_RATE = 16000\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Load WavLM\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "wavlm = WavLMModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "wavlm.eval()\n",
    "for p in wavlm.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "def load_wav_16k(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Robust WAV loader for Windows (no torchaudio, no TorchCodec).\n",
    "    Returns mono float32 waveform at 16 kHz.\n",
    "    \"\"\"\n",
    "    wav, sr = sf.read(path, always_2d=False)\n",
    "\n",
    "    # Ensure mono\n",
    "    if wav.ndim > 1:\n",
    "        wav = np.mean(wav, axis=1)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != SAMPLE_RATE:\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
    "\n",
    "    # Normalize\n",
    "    wav = wav / (np.max(np.abs(wav)) + 1e-9)\n",
    "\n",
    "    return wav.astype(np.float32)\n"
   ],
   "id": "fe30928e09fa46d5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:55:59.906770Z",
     "start_time": "2026-01-04T21:55:59.901063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def wavlm_embedding(wav_1d: np.ndarray) -> np.ndarray:\n",
    "    inputs = feature_extractor(\n",
    "        wav_1d,\n",
    "        sampling_rate=SAMPLE_RATE,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False\n",
    "    )\n",
    "    input_values = inputs[\"input_values\"].to(DEVICE)  # (1, L)\n",
    "\n",
    "    out = wavlm(input_values=input_values)\n",
    "    h = out.last_hidden_state  # (1, T, 768)\n",
    "\n",
    "    mu = h.mean(dim=1)         # (1, 768)\n",
    "    sigma = h.std(dim=1)       # (1, 768)\n",
    "    emb = torch.cat([mu, sigma], dim=1)  # (1, 1536)\n",
    "\n",
    "    return emb.squeeze(0).cpu().numpy().astype(np.float32)\n"
   ],
   "id": "8074c324b9787784",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T21:56:02.084699Z",
     "start_time": "2026-01-04T21:56:02.078647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, torch\n",
    "torch.set_num_threads(max(1, os.cpu_count() - 1))\n"
   ],
   "id": "f74a5009a471e99a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:17:51.717767Z",
     "start_time": "2026-01-05T13:17:51.702833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "EMB_TRAIN_DIR = \"wavlm_emb_train\"\n",
    "EMB_VAL_DIR   = \"wavlm_emb_val\"\n",
    "EMB_TEST_DIR  = \"wavlm_emb_test\"\n",
    "os.makedirs(EMB_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(EMB_VAL_DIR, exist_ok=True)\n",
    "os.makedirs(EMB_TEST_DIR, exist_ok=True)\n",
    "\n",
    "def precompute_embeddings(wav_paths, out_dir, log_every=50):\n",
    "    saved = 0\n",
    "    for i, wav_path in enumerate(wav_paths, start=1):\n",
    "        base = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "        out_path = os.path.join(out_dir, base + \".npy\")\n",
    "\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            w = load_wav_16k(wav_path)\n",
    "            emb = wavlm_embedding(w)          # (1536,)\n",
    "            np.save(out_path, emb)\n",
    "            saved += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Skip {wav_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if i % log_every == 0:\n",
    "            print(f\"[{out_dir}] {i}/{len(wav_paths)} processed | {saved} saved\")\n",
    "\n",
    "    print(f\"Done {out_dir}: saved {saved} embeddings\")\n"
   ],
   "id": "59d2219565c67ab0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-04T22:46:39.911266Z",
     "start_time": "2026-01-04T21:56:07.244084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "precompute_embeddings(train_paths, EMB_TRAIN_DIR)\n",
    "precompute_embeddings(val_paths,   EMB_VAL_DIR)\n",
    "precompute_embeddings(test_paths,  EMB_TEST_DIR)\n"
   ],
   "id": "311dcfa9e118d911",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wavlm_emb_train] 50/5141 processed | 50 saved\n",
      "[wavlm_emb_train] 100/5141 processed | 100 saved\n",
      "[wavlm_emb_train] 150/5141 processed | 150 saved\n",
      "[wavlm_emb_train] 200/5141 processed | 200 saved\n",
      "[wavlm_emb_train] 250/5141 processed | 250 saved\n",
      "[wavlm_emb_train] 300/5141 processed | 300 saved\n",
      "[wavlm_emb_train] 350/5141 processed | 350 saved\n",
      "[wavlm_emb_train] 400/5141 processed | 400 saved\n",
      "[wavlm_emb_train] 450/5141 processed | 450 saved\n",
      "[wavlm_emb_train] 500/5141 processed | 500 saved\n",
      "[wavlm_emb_train] 550/5141 processed | 550 saved\n",
      "[wavlm_emb_train] 600/5141 processed | 600 saved\n",
      "[wavlm_emb_train] 650/5141 processed | 650 saved\n",
      "[wavlm_emb_train] 700/5141 processed | 700 saved\n",
      "[wavlm_emb_train] 750/5141 processed | 750 saved\n",
      "[wavlm_emb_train] 800/5141 processed | 800 saved\n",
      "[wavlm_emb_train] 850/5141 processed | 850 saved\n",
      "[wavlm_emb_train] 900/5141 processed | 900 saved\n",
      "[wavlm_emb_train] 950/5141 processed | 950 saved\n",
      "[wavlm_emb_train] 1000/5141 processed | 1000 saved\n",
      "[wavlm_emb_train] 1050/5141 processed | 1050 saved\n",
      "[wavlm_emb_train] 1100/5141 processed | 1100 saved\n",
      "[wavlm_emb_train] 1150/5141 processed | 1150 saved\n",
      "[wavlm_emb_train] 1200/5141 processed | 1200 saved\n",
      "[wavlm_emb_train] 1250/5141 processed | 1250 saved\n",
      "[wavlm_emb_train] 1300/5141 processed | 1300 saved\n",
      "[wavlm_emb_train] 1350/5141 processed | 1350 saved\n",
      "[wavlm_emb_train] 1400/5141 processed | 1400 saved\n",
      "[wavlm_emb_train] 1450/5141 processed | 1450 saved\n",
      "[wavlm_emb_train] 1500/5141 processed | 1500 saved\n",
      "[wavlm_emb_train] 1550/5141 processed | 1550 saved\n",
      "[wavlm_emb_train] 1600/5141 processed | 1600 saved\n",
      "[wavlm_emb_train] 1650/5141 processed | 1650 saved\n",
      "[wavlm_emb_train] 1700/5141 processed | 1700 saved\n",
      "[wavlm_emb_train] 1750/5141 processed | 1750 saved\n",
      "[wavlm_emb_train] 1800/5141 processed | 1800 saved\n",
      "[wavlm_emb_train] 1850/5141 processed | 1850 saved\n",
      "[wavlm_emb_train] 1900/5141 processed | 1900 saved\n",
      "[wavlm_emb_train] 1950/5141 processed | 1950 saved\n",
      "[wavlm_emb_train] 2000/5141 processed | 2000 saved\n",
      "[wavlm_emb_train] 2050/5141 processed | 2050 saved\n",
      "[wavlm_emb_train] 2100/5141 processed | 2100 saved\n",
      "[wavlm_emb_train] 2150/5141 processed | 2150 saved\n",
      "[wavlm_emb_train] 2200/5141 processed | 2200 saved\n",
      "[wavlm_emb_train] 2250/5141 processed | 2250 saved\n",
      "[wavlm_emb_train] 2300/5141 processed | 2300 saved\n",
      "[wavlm_emb_train] 2350/5141 processed | 2350 saved\n",
      "[wavlm_emb_train] 2400/5141 processed | 2400 saved\n",
      "[wavlm_emb_train] 2450/5141 processed | 2450 saved\n",
      "[wavlm_emb_train] 2500/5141 processed | 2500 saved\n",
      "[wavlm_emb_train] 2550/5141 processed | 2550 saved\n",
      "[wavlm_emb_train] 2600/5141 processed | 2600 saved\n",
      "[wavlm_emb_train] 2650/5141 processed | 2650 saved\n",
      "[wavlm_emb_train] 2700/5141 processed | 2700 saved\n",
      "[wavlm_emb_train] 2750/5141 processed | 2750 saved\n",
      "[wavlm_emb_train] 2800/5141 processed | 2800 saved\n",
      "[wavlm_emb_train] 2850/5141 processed | 2850 saved\n",
      "[wavlm_emb_train] 2900/5141 processed | 2900 saved\n",
      "[wavlm_emb_train] 2950/5141 processed | 2950 saved\n",
      "[wavlm_emb_train] 3000/5141 processed | 3000 saved\n",
      "[wavlm_emb_train] 3050/5141 processed | 3050 saved\n",
      "[wavlm_emb_train] 3100/5141 processed | 3100 saved\n",
      "[wavlm_emb_train] 3150/5141 processed | 3150 saved\n",
      "[wavlm_emb_train] 3200/5141 processed | 3200 saved\n",
      "[wavlm_emb_train] 3250/5141 processed | 3250 saved\n",
      "[wavlm_emb_train] 3300/5141 processed | 3300 saved\n",
      "[wavlm_emb_train] 3350/5141 processed | 3350 saved\n",
      "[wavlm_emb_train] 3400/5141 processed | 3400 saved\n",
      "[wavlm_emb_train] 3450/5141 processed | 3450 saved\n",
      "[wavlm_emb_train] 3500/5141 processed | 3500 saved\n",
      "[wavlm_emb_train] 3550/5141 processed | 3550 saved\n",
      "[wavlm_emb_train] 3600/5141 processed | 3600 saved\n",
      "[wavlm_emb_train] 3650/5141 processed | 3650 saved\n",
      "[wavlm_emb_train] 3700/5141 processed | 3700 saved\n",
      "[wavlm_emb_train] 3750/5141 processed | 3750 saved\n",
      "[wavlm_emb_train] 3800/5141 processed | 3800 saved\n",
      "[wavlm_emb_train] 3850/5141 processed | 3850 saved\n",
      "[wavlm_emb_train] 3900/5141 processed | 3900 saved\n",
      "[wavlm_emb_train] 3950/5141 processed | 3950 saved\n",
      "[wavlm_emb_train] 4000/5141 processed | 4000 saved\n",
      "[wavlm_emb_train] 4050/5141 processed | 4050 saved\n",
      "[wavlm_emb_train] 4100/5141 processed | 4100 saved\n",
      "[wavlm_emb_train] 4150/5141 processed | 4150 saved\n",
      "[wavlm_emb_train] 4200/5141 processed | 4200 saved\n",
      "[wavlm_emb_train] 4250/5141 processed | 4250 saved\n",
      "[wavlm_emb_train] 4300/5141 processed | 4300 saved\n",
      "[wavlm_emb_train] 4350/5141 processed | 4350 saved\n",
      "[wavlm_emb_train] 4400/5141 processed | 4400 saved\n",
      "[wavlm_emb_train] 4450/5141 processed | 4450 saved\n",
      "[wavlm_emb_train] 4500/5141 processed | 4500 saved\n",
      "[wavlm_emb_train] 4550/5141 processed | 4550 saved\n",
      "[wavlm_emb_train] 4600/5141 processed | 4600 saved\n",
      "[wavlm_emb_train] 4650/5141 processed | 4650 saved\n",
      "[wavlm_emb_train] 4700/5141 processed | 4700 saved\n",
      "[wavlm_emb_train] 4750/5141 processed | 4750 saved\n",
      "[wavlm_emb_train] 4800/5141 processed | 4800 saved\n",
      "[wavlm_emb_train] 4850/5141 processed | 4850 saved\n",
      "[wavlm_emb_train] 4900/5141 processed | 4900 saved\n",
      "[wavlm_emb_train] 4950/5141 processed | 4950 saved\n",
      "[wavlm_emb_train] 5000/5141 processed | 5000 saved\n",
      "[wavlm_emb_train] 5050/5141 processed | 5050 saved\n",
      "[wavlm_emb_train] 5100/5141 processed | 5100 saved\n",
      "Done wavlm_emb_train: saved 5141 embeddings\n",
      "[wavlm_emb_val] 50/1066 processed | 50 saved\n",
      "[wavlm_emb_val] 100/1066 processed | 100 saved\n",
      "[wavlm_emb_val] 150/1066 processed | 150 saved\n",
      "[wavlm_emb_val] 200/1066 processed | 200 saved\n",
      "[wavlm_emb_val] 250/1066 processed | 250 saved\n",
      "[wavlm_emb_val] 300/1066 processed | 300 saved\n",
      "[wavlm_emb_val] 350/1066 processed | 350 saved\n",
      "[wavlm_emb_val] 400/1066 processed | 400 saved\n",
      "[wavlm_emb_val] 450/1066 processed | 450 saved\n",
      "[wavlm_emb_val] 500/1066 processed | 500 saved\n",
      "[wavlm_emb_val] 550/1066 processed | 550 saved\n",
      "[wavlm_emb_val] 600/1066 processed | 600 saved\n",
      "[wavlm_emb_val] 650/1066 processed | 650 saved\n",
      "[wavlm_emb_val] 700/1066 processed | 700 saved\n",
      "[wavlm_emb_val] 750/1066 processed | 750 saved\n",
      "[wavlm_emb_val] 800/1066 processed | 800 saved\n",
      "[wavlm_emb_val] 850/1066 processed | 850 saved\n",
      "[wavlm_emb_val] 900/1066 processed | 900 saved\n",
      "[wavlm_emb_val] 950/1066 processed | 950 saved\n",
      "[wavlm_emb_val] 1000/1066 processed | 1000 saved\n",
      "[wavlm_emb_val] 1050/1066 processed | 1050 saved\n",
      "Done wavlm_emb_val: saved 1066 embeddings\n",
      "[wavlm_emb_test] 50/1224 processed | 50 saved\n",
      "[wavlm_emb_test] 100/1224 processed | 100 saved\n",
      "[wavlm_emb_test] 150/1224 processed | 150 saved\n",
      "[wavlm_emb_test] 200/1224 processed | 200 saved\n",
      "[wavlm_emb_test] 250/1224 processed | 250 saved\n",
      "[wavlm_emb_test] 300/1224 processed | 300 saved\n",
      "[wavlm_emb_test] 350/1224 processed | 350 saved\n",
      "[wavlm_emb_test] 400/1224 processed | 400 saved\n",
      "[wavlm_emb_test] 450/1224 processed | 450 saved\n",
      "[wavlm_emb_test] 500/1224 processed | 500 saved\n",
      "[wavlm_emb_test] 550/1224 processed | 550 saved\n",
      "[wavlm_emb_test] 600/1224 processed | 600 saved\n",
      "[wavlm_emb_test] 650/1224 processed | 650 saved\n",
      "[wavlm_emb_test] 700/1224 processed | 700 saved\n",
      "[wavlm_emb_test] 750/1224 processed | 750 saved\n",
      "[wavlm_emb_test] 800/1224 processed | 800 saved\n",
      "[wavlm_emb_test] 850/1224 processed | 850 saved\n",
      "[wavlm_emb_test] 900/1224 processed | 900 saved\n",
      "[wavlm_emb_test] 950/1224 processed | 950 saved\n",
      "[wavlm_emb_test] 1000/1224 processed | 1000 saved\n",
      "[wavlm_emb_test] 1050/1224 processed | 1050 saved\n",
      "[wavlm_emb_test] 1100/1224 processed | 1100 saved\n",
      "[wavlm_emb_test] 1150/1224 processed | 1150 saved\n",
      "[wavlm_emb_test] 1200/1224 processed | 1200 saved\n",
      "Done wavlm_emb_test: saved 1224 embeddings\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:18:06.243099Z",
     "start_time": "2026-01-05T13:18:06.182482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fn = next(f for f in os.listdir(EMB_TRAIN_DIR) if f.endswith(\".npy\"))\n",
    "x = np.load(os.path.join(EMB_TRAIN_DIR, fn))\n",
    "print(x.shape)   # (1536,)\n"
   ],
   "id": "e8f38f61954c28b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:18:22.735731Z",
     "start_time": "2026-01-05T13:18:22.679241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, wav_paths, emb_dir):\n",
    "        self.items = []\n",
    "        for wp in wav_paths:\n",
    "            base = os.path.splitext(os.path.basename(wp))[0]\n",
    "            emb_path = os.path.join(emb_dir, base + \".npy\")\n",
    "            _, label = parse_actor_and_emotion(base + \".wav\")\n",
    "            self.items.append((emb_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emb_path, label = self.items[idx]\n",
    "        x = np.load(emb_path).astype(np.float32)   # (1536,)\n",
    "        return torch.from_numpy(x), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(EmbDataset(train_paths, EMB_TRAIN_DIR), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(EmbDataset(val_paths,   EMB_VAL_DIR),   batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(EmbDataset(test_paths,  EMB_TEST_DIR),  batch_size=128, shuffle=False)\n"
   ],
   "id": "26a8199cc39b689a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:18:25.695039Z",
     "start_time": "2026-01-05T13:18:24.998406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "print(xb.shape, yb[:5])  # torch.Size([64, 1536])\n"
   ],
   "id": "b07692fa6b2e96dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1536]) tensor([5, 5, 0, 3, 0])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:21:46.745Z",
     "start_time": "2026-01-05T13:18:30.586269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_dim=1536, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item() * x.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_true.append(y.cpu().numpy())\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_true)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return acc, f1, y_true, y_pred\n",
    "\n",
    "model = MLPClassifier(in_dim=1536, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "best_f1 = -1.0\n",
    "patience = 8\n",
    "wait = 0\n",
    "best_path = \"best_wavlm_mlp.pt\"\n",
    "\n",
    "for epoch in range(1, 81):\n",
    "    tr_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_acc, val_f1, _, _ = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch:03d} | train_loss {tr_loss:.4f} | val_acc {val_acc:.4f} | val_f1 {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_f1 + 1e-4:\n",
    "        best_f1 = val_f1\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping. Best val_f1={best_f1:.4f}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "val_acc, val_f1, y_true, y_pred = evaluate(model, val_loader)\n",
    "print(f\"BEST | val_acc={val_acc:.4f} val_f1={val_f1:.4f}\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ],
   "id": "161914797c67e6f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss 1.6394 | val_acc 0.4812 | val_f1 0.4307\n",
      "Epoch 002 | train_loss 1.4711 | val_acc 0.4841 | val_f1 0.4447\n",
      "Epoch 003 | train_loss 1.3780 | val_acc 0.5403 | val_f1 0.5138\n",
      "Epoch 004 | train_loss 1.3230 | val_acc 0.5675 | val_f1 0.5532\n",
      "Epoch 005 | train_loss 1.2758 | val_acc 0.5844 | val_f1 0.5756\n",
      "Epoch 006 | train_loss 1.2636 | val_acc 0.5854 | val_f1 0.5777\n",
      "Epoch 007 | train_loss 1.2406 | val_acc 0.6154 | val_f1 0.6242\n",
      "Epoch 008 | train_loss 1.2189 | val_acc 0.6454 | val_f1 0.6469\n",
      "Epoch 009 | train_loss 1.2116 | val_acc 0.6229 | val_f1 0.6278\n",
      "Epoch 010 | train_loss 1.1960 | val_acc 0.6473 | val_f1 0.6496\n",
      "Epoch 011 | train_loss 1.1871 | val_acc 0.6370 | val_f1 0.6417\n",
      "Epoch 012 | train_loss 1.1782 | val_acc 0.6229 | val_f1 0.6293\n",
      "Epoch 013 | train_loss 1.1714 | val_acc 0.6210 | val_f1 0.6086\n",
      "Epoch 014 | train_loss 1.1729 | val_acc 0.6220 | val_f1 0.6282\n",
      "Epoch 015 | train_loss 1.1653 | val_acc 0.6379 | val_f1 0.6372\n",
      "Epoch 016 | train_loss 1.1588 | val_acc 0.6604 | val_f1 0.6581\n",
      "Epoch 017 | train_loss 1.1653 | val_acc 0.6379 | val_f1 0.6362\n",
      "Epoch 018 | train_loss 1.1665 | val_acc 0.6501 | val_f1 0.6537\n",
      "Epoch 019 | train_loss 1.1506 | val_acc 0.6604 | val_f1 0.6617\n",
      "Epoch 020 | train_loss 1.1362 | val_acc 0.6670 | val_f1 0.6621\n",
      "Epoch 021 | train_loss 1.1340 | val_acc 0.6426 | val_f1 0.6345\n",
      "Epoch 022 | train_loss 1.1459 | val_acc 0.6614 | val_f1 0.6575\n",
      "Epoch 023 | train_loss 1.1313 | val_acc 0.6567 | val_f1 0.6567\n",
      "Epoch 024 | train_loss 1.1410 | val_acc 0.6745 | val_f1 0.6759\n",
      "Epoch 025 | train_loss 1.1298 | val_acc 0.6698 | val_f1 0.6728\n",
      "Epoch 026 | train_loss 1.1174 | val_acc 0.6313 | val_f1 0.6363\n",
      "Epoch 027 | train_loss 1.1365 | val_acc 0.6520 | val_f1 0.6518\n",
      "Epoch 028 | train_loss 1.1265 | val_acc 0.6698 | val_f1 0.6720\n",
      "Epoch 029 | train_loss 1.1284 | val_acc 0.6510 | val_f1 0.6503\n",
      "Epoch 030 | train_loss 1.1254 | val_acc 0.6482 | val_f1 0.6422\n",
      "Epoch 031 | train_loss 1.1144 | val_acc 0.6529 | val_f1 0.6502\n",
      "Epoch 032 | train_loss 1.1132 | val_acc 0.6614 | val_f1 0.6653\n",
      "Early stopping. Best val_f1=0.6759\n",
      "BEST | val_acc=0.6745 val_f1=0.6759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8698    0.8077    0.8376       182\n",
      "           1     0.6353    0.5934    0.6136       182\n",
      "           2     0.6382    0.5330    0.5808       182\n",
      "           3     0.5591    0.7802    0.6514       182\n",
      "           4     0.7669    0.8013    0.7837       156\n",
      "           5     0.6329    0.5495    0.5882       182\n",
      "\n",
      "    accuracy                         0.6745      1066\n",
      "   macro avg     0.6837    0.6775    0.6759      1066\n",
      "weighted avg     0.6817    0.6745    0.6733      1066\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:30:43.395683Z",
     "start_time": "2026-01-05T13:30:43.384959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FINAL_PATH = \"wavlm_mlp_emotion.pt\"\n",
    "\n",
    "# model should already be the best one (or load best_path first)\n",
    "torch.save(model.state_dict(), FINAL_PATH)\n",
    "print(\"Saved weights to:\", FINAL_PATH)\n"
   ],
   "id": "b7c5b329e97c559",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights to: wavlm_mlp_emotion.pt\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:23:03.085607Z",
     "start_time": "2026-01-05T13:23:03.067626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "model.eval()\n"
   ],
   "id": "b6a63bd012f2b59e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(\n",
       "  (net): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1536, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:23:06.765295Z",
     "start_time": "2026-01-05T13:23:04.742144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "test_acc, test_f1, y_true, y_pred = evaluate(model, test_loader)\n",
    "\n",
    "print(f\"TEST | acc={test_acc:.4f} | macro_f1={test_f1:.4f}\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=[\"ANG\",\"DIS\",\"FEA\",\"HAP\",\"NEU\",\"SAD\"],\n",
    "    digits=4\n",
    "))\n"
   ],
   "id": "491d1c22d8b6af0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST | acc=0.6258 | macro_f1=0.6234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG     0.7067    0.7608    0.7327       209\n",
      "         DIS     0.5525    0.6794    0.6094       209\n",
      "         FEA     0.6460    0.4976    0.5622       209\n",
      "         HAP     0.5669    0.6890    0.6220       209\n",
      "         NEU     0.6910    0.6872    0.6891       179\n",
      "         SAD     0.6309    0.4498    0.5251       209\n",
      "\n",
      "    accuracy                         0.6258      1224\n",
      "   macro avg     0.6323    0.6273    0.6234      1224\n",
      "weighted avg     0.6309    0.6258    0.6218      1224\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:23:18.617884Z",
     "start_time": "2026-01-05T13:23:18.606168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# y_true: ground truth emotion labels\n",
    "# y_pred: predicted emotion labels\n",
    "\n",
    "uar = recall_score(y_true, y_pred, average='macro')\n",
    "print(f\"UAR: {uar:.4f}\")\n"
   ],
   "id": "f8425dca54ff68b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR: 0.6273\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:23:22.050537Z",
     "start_time": "2026-01-05T13:23:22.035407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# overall UAR\n",
    "uar = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# per-class recall\n",
    "recalls = recall_score(y_true, y_pred, average=None)\n",
    "\n",
    "print(f\"UAR: {uar:.4f}\")\n",
    "print(\"Per-class recall:\")\n",
    "for i, r in enumerate(recalls):\n",
    "    print(f\"Class {i}: {r:.3f}\")\n"
   ],
   "id": "15bfbf1500103184",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR: 0.6273\n",
      "Per-class recall:\n",
      "Class 0: 0.761\n",
      "Class 1: 0.679\n",
      "Class 2: 0.498\n",
      "Class 3: 0.689\n",
      "Class 4: 0.687\n",
      "Class 5: 0.450\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T13:23:25.116276Z",
     "start_time": "2026-01-05T13:23:25.105866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wa = accuracy_score(y_true, y_pred)\n",
    "print(f\"Weighted Accuracy (WA): {wa:.4f}\")\n"
   ],
   "id": "37ca040c32beb406",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy (WA): 0.6258\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
