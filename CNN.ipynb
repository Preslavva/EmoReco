{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading the data",
   "id": "a4cc687af343290d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this step I load only the modelling data which is the cleaned dataset without the demo set so that makes it 7431 audio recordings in total.",
   "id": "7b5301dbc362bafc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:43:59.922965Z",
     "start_time": "2025-12-21T19:43:59.898008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"modelling_metadata.csv\")\n",
    "df.head()\n",
    "df.shape"
   ],
   "id": "89dacde0d00eb304",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7431, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and configuration",
   "id": "31ae9e3736f8745b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:47:56.118362Z",
     "start_time": "2025-12-21T18:47:56.099771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Audio settings\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_SEC = 3.0\n",
    "TARGET_SAMPLES = int(SAMPLE_RATE * DURATION_SEC)\n",
    "\n",
    "# Spectrogram settings\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "N_MELS = 128\n",
    "\n",
    "# Expected time frames\n",
    "TARGET_FRAMES = 1 + (TARGET_SAMPLES // HOP_LENGTH)\n",
    "\n",
    "# Output folders\n",
    "OUT_NPY_DIR = \"mel_npy\"\n",
    "OUT_PNG_DIR = \"mel_png\"\n",
    "os.makedirs(OUT_NPY_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_PNG_DIR, exist_ok=True)"
   ],
   "id": "73ecfb68d4f2810e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this step, I define the audio and spectrogram configuration used to convert speech into fixed-size log-mel spectrograms for a CNN + BiLSTM model. I resample all audio to 16 kHz and trim or pad it to a duration of 3 seconds so that every input has the same length. I compute log-mel spectrograms using 128 mel bands and a fixed hop length, which ensures a consistent time dimension required by the BiLSTM. I also create output directories for saving spectrograms as .npy files for training and .png files for visualization, ensuring a clean and reproducible data preparation pipeline.",
   "id": "e4b8522004861854"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading and normalizing audio length",
   "id": "d6123b00f86af13f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:48:00.675788Z",
     "start_time": "2025-12-21T18:48:00.658751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def load_and_fix_length(path, sample_rate=SAMPLE_RATE, target_samples=TARGET_SAMPLES):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "\n",
    "    # ensure mono audio\n",
    "    if y.ndim > 1:\n",
    "        y = np.mean(y, axis=0)\n",
    "\n",
    "    # resampling only if needed\n",
    "    if sr != sample_rate:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=sample_rate)\n",
    "\n",
    "    # padding or trimming to fixed length\n",
    "    if len(y) < target_samples:\n",
    "        y = np.pad(y, (0, target_samples - len(y)), mode=\"constant\")\n",
    "    else:\n",
    "        y = y[:target_samples]\n",
    "\n",
    "    return y\n"
   ],
   "id": "6342b18c1ac7d677",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use this function to load each audio file, convert it to mono if necessary, resample it to a fixed sample rate, and pad or trim it to a fixed duration. By enforcing a consistent waveform length, I ensure that all generated log-mel spectrograms have the same shape and are compatible with the CNN + BiLSTM model.",
   "id": "e35a1469f64c949"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why is fixed-length audio required?\n",
    "Neural networks require inputs of consistent shape. By trimming or padding each audio clip to a fixed duration, I ensure that all resulting mel spectrograms have the same time dimension, which simplifies batching and allows the BiLSTM to process temporal information without variable-length handling."
   ],
   "id": "db185bad8d434d83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why stereo audio is converted to mono?\n",
    "Stereo audio is converted to mono to ensure consistent input shape and because spatial information is not relevant for speech emotion recognition. Emotional cues are primarily encoded in pitch, energy and timing rather than channel differences."
   ],
   "id": "2f7210c09222b12e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Converting waveform into log-Mel spectrogram",
   "id": "2b63bca621295666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:48:03.102606Z",
     "start_time": "2025-12-21T18:48:03.091957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def wav_to_logmel(y, sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS):\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # per-sample standardization\n",
    "    S_norm = (S_db - S_db.mean()) / (S_db.std() + 1e-6)\n",
    "\n",
    "    # output shape for PyTorch CNN: (1, n_mels, time)\n",
    "    return S_norm[np.newaxis, :, :].astype(np.float32)\n"
   ],
   "id": "e766aae1b2b5ad30",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use this function to convert a fixed-length audio signal into a log-mel spectrogram that is suitable for CNN + BiLSTM training. By applying log scaling and normalization and returning the spectrogram in a channel-first format, I ensure that all inputs are numerically stable, consistent in shape and directly compatible with PyTorch models.",
   "id": "4587972927235077"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "31db5e6d3e2ae6bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why log scaling is applied?\n",
    "Neural networks require inputs of consistent shape. By trimming or padding each audio clip to a fixed duration, I ensure that all resulting mel spectrograms have the same time dimension, which simplifies batching and allows the BiLSTM to process temporal information without variable-length handling."
   ],
   "id": "84f02357442f2efb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving a PNG preview image",
   "id": "39e5b78797cf0bd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:48:09.885838Z",
     "start_time": "2025-12-21T18:48:07.506457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "def save_logmel_png(mel_img, out_path):\n",
    "    # mel_img shape: (1, n_mels, time)\n",
    "    S_img = np.squeeze(mel_img, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    librosa.display.specshow(S_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(out_path, dpi=150, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close()\n"
   ],
   "id": "50c647a78f93e519",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use this function to save a log-mel spectrogram as a clean PNG image for visualization and inspection. By removing the channel dimension and disabling all axes and padding, I ensure that the saved image contains only the spectrogram content, while keeping the NPY files as the primary input for CNN + BiLSTM training.",
   "id": "9cf8bb136ff49b35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Converting and saving all files",
   "id": "e05d663c0cf97da6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T18:55:46.243686Z",
     "start_time": "2025-12-21T18:48:21.258844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_files = None\n",
    "SAVE_PNG = True\n",
    "LOG_EVERY = 500           # progress print frequency\n",
    "\n",
    "rows = df if max_files is None else df.head(max_files)\n",
    "\n",
    "for i, row in enumerate(rows.itertuples(index=False), start=1):\n",
    "    wav_path = row.file_path\n",
    "    file_name = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "\n",
    "    try:\n",
    "        # loading audio and enforcing fixed length\n",
    "        y = load_and_fix_length(wav_path)\n",
    "\n",
    "        # converting to log-mel in the agreed format (1, n_mels, T)\n",
    "        mel_img = wav_to_logmel(y)\n",
    "\n",
    "        # saving NPY\n",
    "        np.save(os.path.join(OUT_NPY_DIR, f\"{file_name}.npy\"), mel_img)\n",
    "\n",
    "        # saving PNG\n",
    "        if SAVE_PNG:\n",
    "            save_logmel_png(mel_img, os.path.join(OUT_PNG_DIR, f\"{file_name}.png\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file due to error: {wav_path}\\n  -> {e}\")\n",
    "        continue\n",
    "\n",
    "    if i % LOG_EVERY == 0:\n",
    "        print(f\"Processed {i} files...\")\n",
    "\n",
    "print(\"Saved NPY spectrograms to:\", OUT_NPY_DIR)\n",
    "if SAVE_PNG:\n",
    "    print(\"Saved PNG previews to:\", OUT_PNG_DIR)\n"
   ],
   "id": "b63ccfe56739bd4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 files...\n",
      "Processed 1000 files...\n",
      "Processed 1500 files...\n",
      "Processed 2000 files...\n",
      "Processed 2500 files...\n",
      "Processed 3000 files...\n",
      "Processed 3500 files...\n",
      "Processed 4000 files...\n",
      "Processed 4500 files...\n",
      "Processed 5000 files...\n",
      "Processed 5500 files...\n",
      "Processed 6000 files...\n",
      "Processed 6500 files...\n",
      "Processed 7000 files...\n",
      "Saved NPY spectrograms to: mel_npy\n",
      "Saved PNG previews to: mel_png\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use this block to iterate through the dataset, convert each audio file into a fixed-length log-mel spectrogram, and save it as a .npy file for model training, with optional PNG output for visualization. By overwriting or regenerating files in a controlled way, I ensure that all saved spectrograms follow the same preprocessing rules and are fully consistent with the CNN + BiLSTM pipeline.",
   "id": "75186172d206b1d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why are NPY files used instead of images?\n",
    "Spectrograms are saved as NumPy arrays rather than images to preserve exact numerical values and the true temporal structure of the signal. This format is more suitable for sequence-based models such as BiLSTMs and avoids artifacts introduced by image rendering."
   ],
   "id": "ccc1582a8a9e06a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why CNN + BiLSTM is used?\n",
    "The CNN is responsible for learning local spectral patterns from the mel spectrogram, while the BiLSTM models how these patterns evolve over time. This combination allows the system to capture both short-term acoustic features and long-term emotional dynamics."
   ],
   "id": "c4c84d429e5d9ec5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "ae01afdc79871b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:54:09.125146Z",
     "start_time": "2025-12-21T19:54:09.120684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ],
   "id": "f1cb049143ea4ed6",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I import standard Python and PyTorch libraries to handle file paths, numerical data, dataset splitting, and neural network training. NumPy is used to load and manipulate mel spectrograms, while PyTorch provides tensor operations, model definitions and data loading utilities required for training the CNN + BiLSTM model.",
   "id": "3999e2d3815d785d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations",
   "id": "86f8a08723f12081"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:54:34.966731Z",
     "start_time": "2025-12-21T19:54:34.958951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# Emotion mapping\n",
    "EMO_MAP = {\n",
    "    \"ANG\": 0,\n",
    "    \"DIS\": 1,\n",
    "    \"FEA\": 2,\n",
    "    \"HAP\": 3,\n",
    "    \"NEU\": 4,\n",
    "    \"SAD\": 5\n",
    "}\n",
    "NUM_CLASSES = len(EMO_MAP)\n",
    "\n",
    "# Data location\n",
    "NPY_DIR = \"mel_npy\"\n"
   ],
   "id": "7477986555f0bc78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use this block to ensure reproducibility, configure the computation device, and define all global settings required for model training. By fixing random seeds, I make experimental results repeatable, while dynamically selecting the GPU or CPU ensures efficient execution. I also define spectrogram parameters and emotion label mappings that must match the preprocessing stage, providing a consistent and well-organized configuration for training and evaluating the CNN + BiLSTM model on the CREMA-D dataset.",
   "id": "a50041e2372ab1c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collecting all NPY files",
   "id": "92ec655536679ce9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:54:48.803145Z",
     "start_time": "2025-12-21T19:54:48.778094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "npy_paths = sorted([str(p) for p in Path(NPY_DIR).glob(\"*.npy\")])\n",
    "print(\"Found NPY files:\", len(npy_paths))\n"
   ],
   "id": "f2cc0015b7094366",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found NPY files: 7431\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inferring spectrogram shape",
   "id": "12797cc57c3c1898"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:55:18.972977Z",
     "start_time": "2025-12-21T19:55:18.965288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = np.load(npy_paths[0])\n",
    "if example.ndim == 3:\n",
    "    _, N_MELS, T_FRAMES = example.shape\n",
    "else:\n",
    "    N_MELS, T_FRAMES = example.shape\n",
    "\n",
    "print(\"Spectrogram shape:\", (1, N_MELS, T_FRAMES))\n"
   ],
   "id": "b163016f732e31a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram shape: (1, 128, 188)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I load one example spectrogram to infer the number of mel bands and time frames, ensuring that the model architecture is configured to exactly match the precomputed features.",
   "id": "dc6452ff843388b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The spectrogram shape (1, 128, 188) represents a single-channel log-mel spectrogram with 128 frequency bands observed over 188 time steps, giving the model both spectral and temporal information needed for emotion recognition.",
   "id": "933c650eebf7519d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parsing actor ID and emotion from filename",
   "id": "5e35c9fbe5c4f94f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:55:55.633452Z",
     "start_time": "2025-12-21T19:55:55.626156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_actor_and_emotion(filename: str):\n",
    "\n",
    "    # Actor ID = first number in filename\n",
    "    actor_match = re.search(r\"\\d+\", filename)\n",
    "    if actor_match is None:\n",
    "        raise ValueError(f\"No actor ID in {filename}\")\n",
    "    actor_id = actor_match.group(0)\n",
    "\n",
    "    # Emotion code\n",
    "    emotion = None\n",
    "    for emo in EMO_MAP:\n",
    "        if f\"_{emo}_\" in filename or filename.startswith(f\"{emo}_\"):\n",
    "            emotion = EMO_MAP[emo]\n",
    "            break\n",
    "\n",
    "    if emotion is None:\n",
    "        raise ValueError(f\"No emotion code in {filename}\")\n",
    "\n",
    "    return actor_id, emotion\n"
   ],
   "id": "dd778460da98b015",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Actor-independent split",
   "id": "ca6dd53730052ebf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:56:57.911625Z",
     "start_time": "2025-12-21T19:56:57.904933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def actor_independent_split(paths, train_ratio=0.7, val_ratio=0.15):\n",
    "    by_actor = {}\n",
    "\n",
    "    for p in paths:\n",
    "        fname = os.path.basename(p)\n",
    "        actor_id, _ = parse_actor_and_emotion(fname)\n",
    "        by_actor.setdefault(actor_id, []).append(p)\n",
    "\n",
    "    actors = list(by_actor.keys())\n",
    "    random.shuffle(actors)\n",
    "\n",
    "    n = len(actors)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    train_actors = set(actors[:n_train])\n",
    "    val_actors   = set(actors[n_train:n_train+n_val])\n",
    "    test_actors  = set(actors[n_train+n_val:])\n",
    "\n",
    "    def collect(actor_set):\n",
    "        out = []\n",
    "        for a in actor_set:\n",
    "            out.extend(by_actor[a])\n",
    "        return out\n",
    "\n",
    "    return (\n",
    "        collect(train_actors),\n",
    "        collect(val_actors),\n",
    "        collect(test_actors)\n",
    "    )\n"
   ],
   "id": "92335e25d0c4c2ec",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T19:57:06.811843Z",
     "start_time": "2025-12-21T19:57:06.777513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_paths, val_paths, test_paths = actor_independent_split(npy_paths)\n",
    "print(len(train_paths), len(val_paths), len(test_paths))\n"
   ],
   "id": "1a25934f05d58078",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5141 1066 1224\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use an actor-independent splitting strategy to divide the dataset into training, validation, and test sets. By grouping samples by speaker and ensuring that no actor appears in more than one split, I prevent speaker leakage and obtain a more realistic evaluation of how well the model generalizes to unseen speakers.",
   "id": "9ff4bb460d5c3744"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PyTorch Dataset",
   "id": "b19ef616422a0845"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T20:53:34.364987Z",
     "start_time": "2025-12-21T20:53:34.349925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MelNPYDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        fname = os.path.basename(path)\n",
    "\n",
    "        # loading spectrogram\n",
    "        spec = np.load(path).astype(np.float32)\n",
    "        if spec.ndim == 2:\n",
    "            spec = spec[np.newaxis, :, :]\n",
    "\n",
    "        # parsing label from filename\n",
    "        _, label = parse_actor_and_emotion(fname)\n",
    "\n",
    "        x = torch.from_numpy(spec)\n",
    "        y = torch.tensor(label, dtype=torch.long)\n",
    "        return x, y\n"
   ],
   "id": "7a24365628083191",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I use this Dataset class to load precomputed mel spectrograms from .npy files and extract emotion labels directly from their filenames. Each sample is returned in channel-first format along with its corresponding class label, enabling efficient training without relying on external metadata",
   "id": "5fc13087501cfb70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataloaders",
   "id": "bbf1e2887faa08a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    MelNPYDataset(train_paths),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    MelNPYDataset(val_paths),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    MelNPYDataset(test_paths),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n"
   ],
   "id": "d596f127ce9b563e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block of code controls how the data is given to the model while it is training and being evaluated. It does not train the model itself. Instead, it decides how many samples are used at a time, in what order they are read, and which data is used for training, validation, and testing.",
   "id": "9d5445f987287b3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What BATCH_SIZE = 32 means?\n",
    "Setting the batch size to 32 means that the model processes 32 audio samples at once.\n",
    "Instead of learning from one spectrogram at a time, the model learns from small groups of samples.\n",
    "\n",
    "After processing one batch, the model updates its internal weights. Then it moves on to the next batch, until it has seen the entire dataset."
   ],
   "id": "9cbf2296edca3935"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What is a DataLoader?\n",
    "A DataLoader is a tool that reads data from the dataset, groups samples into batches, optionally shuffles the order of samples and feeds the marches to the model during training."
   ],
   "id": "d56de8022d0c6daa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T21:20:35.531134Z",
     "start_time": "2025-12-21T21:20:35.515065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNBiLSTM(nn.Module):\n",
    "    def __init__(self, lstm_hidden=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1)),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1))\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, N_MELS, T_FRAMES)\n",
    "            z = self.cnn(dummy)\n",
    "            C, Fp, Tp = z.shape[1], z.shape[2], z.shape[3]\n",
    "            lstm_in = C * Fp\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            lstm_in,\n",
    "            lstm_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2*lstm_hidden, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)\n",
    "        z = z.permute(0, 3, 1, 2)   # (B, T, C, F)\n",
    "        z = z.flatten(2)            # (B, T, C*F)\n",
    "        out, _ = self.lstm(z)\n",
    "        return self.fc(out[:, -1])\n"
   ],
   "id": "b9185e515034227f",
   "outputs": [],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
