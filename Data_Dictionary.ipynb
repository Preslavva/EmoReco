{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speech Emotion Recognition using CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset)",
   "id": "b852fef1643ec199"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Project objective",
   "id": "9b1e7b78dbf5487a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The objective of this project is to build a speech emotion recognition system that automatically classifies human emotional states from talking signals, using the CREMA-D dataset. I will take raw speech audio as input, extract meaningful features and use a trained model to predict one of the six emotions: anger, disgust, fear, happy, neutral, sad.",
   "id": "39030d2f004e4425"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Source\n",
    "The CREMA-D dataset was developed by West Chester University of Pennsylvania (WCU), USA\n",
    "(in collaboration with other U.S. research institutions) and consists of American English speech recordings collected from actors of diverse demographic backgrounds.\n",
    "\n",
    "The audio data used in this project originates from the CREMA-D (Crowd-sourced Emotional Multimodal Actors Dataset).\n",
    "\n",
    "Original repository:\n",
    "https://github.com/CheyneyComputerScience/CREMA-D"
   ],
   "id": "8e661cac3a327e51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Dictionary",
   "id": "1a469cf49bd0bd21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified).\n",
    "\n",
    "Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral and Sad) and four different emotion levels (Low, Medium, High and Unspecified)."
   ],
   "id": "4061bed94f2d904c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Column Name     | Data Type  | Description                                                                                                                          |\n",
    "| --------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `file_name`     | `string`   | Name of the WAV file as stored in the dataset (e.g. `1001_DFA_ANG_MD.wav`).                                                          |\n",
    "| `file_path`     | `string`   | Relative or absolute path to the audio file used for loading the WAV data.                                                           |\n",
    "| `actor_id`      | `int`      | Unique 4-digit identifier of the actor who spoke the sentence. Extracted from the filename.                                          |\n",
    "| `sentence_id`   | `string`   | Identifier of the sentence spoken in the recording (e.g. `DFA`, `IEO`).                                                              |\n",
    "| `emotion`       | `category` | Intended emotion encoded in the filename: `ANG` (Anger), `DIS` (Disgust), `FEA` (Fear), `HAP` (Happy), `NEU` (Neutral), `SAD` (Sad). |\n",
    "| `emotion_id`    | `int`      | Integer-encoded emotion label used for machine learning. Mapping: `ANG=0`, `DIS=1`, `FEA=2`, `HAP=3`, `NEU=4`, `SAD=5`.              |\n",
    "| `emotion_level` | `category` | Intended emotion intensity level: `LO` (Low), `MD` (Medium), `HI` (High), `XX` (Unspecified).                                        |\n",
    "| `duration_sec`  | `float`    | Duration of the audio clip in seconds, computed as `num_samples / sample_rate`.                                                      |\n",
    "| `sample_rate`   | `int`      | Sampling rate of the audio file in Hertz (samples per second), e.g. `16000`.                                                         |\n",
    "| `num_samples`   | `int`      | Total number of audio samples in the clip (length of the waveform array).                                                            |\n"
   ],
   "id": "b9bf37b5951ab16d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Small look up table that shows what the sentence codes stand for",
   "id": "878182b01d94b9b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Sentence Code | Full Sentence                         |\n",
    "| ------------- | ------------------------------------- |\n",
    "| `IEO`         | It’s eleven o’clock                   |\n",
    "| `TIE`         | That is exactly what happened         |\n",
    "| `IOM`         | I’m on my way to the meeting          |\n",
    "| `IWW`         | I wonder what this is about           |\n",
    "| `TAI`         | The airplane is almost full           |\n",
    "| `MTI`         | Maybe tomorrow it will be cold        |\n",
    "| `IWL`         | I would like a new alarm clock        |\n",
    "| `ITH`         | I think I have a doctor’s appointment |\n",
    "| `DFA`         | Don’t forget a jacket                 |\n",
    "| `ITS`         | I think I’ve seen this before         |\n",
    "| `TSI`         | The surface is slick                  |\n",
    "| `WSI`         | We’ll stop in a couple of minutes     |\n"
   ],
   "id": "f892f8278f4af25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
